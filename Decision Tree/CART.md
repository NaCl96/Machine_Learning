<!--
 * @Author: your name
 * @Date: 2019-12-21 11:08:16
 * @LastEditTime : 2019-12-21 11:28:03
 * @LastEditors  : Please set LastEditors
 * @Description: In User Settings Edit
 * @FilePath: /Machine_Learning/Decision Tree/CART.md
 -->
#### CART(分类与回归树)
##### 1.1 CART介绍
注意：CART是二叉树！
CART算法：Classification And Regression Tree。顾名思义，CART算法既可以用于创建分类树（Classification Tree），也可以用于创建回归树（Regression Tree）、模型树（Model Tree），两者在建树的过程稍有差异。既可以解决分类问题，也可以解决回归问题。根据某一个维度d和某一个阈值v进行二分，得到的决策树是二叉树。

ID3中使用了信息增益选择特征，增益大优先选择。C4.5中，采用信息增益比选择特征，减少因特征值多导致信息增益大的问题。CART分类树算法使用基尼系数来代替信息增益比，基尼系数代表了模型的不纯度，基尼系数越小，不纯度越低，特征越好。这和信息增益（比）相反。

##### 1.2 基尼指数
基尼指数（基尼不纯度）= 样本被选中的概率 * 样本被分错的概率 有如下公式：
```math
Gini = \sum_{k=1}^kp_k*(1-p_k)=1-\sum_{k=1}^kp_k^2
```
对上述公式进行说明：

- $p_k$表示选中样本属于k的概率，则这个样本被分错的概率为$(1-p_k)$
- 因为样本集合中有k个类别，一个随机选中的样本可以属于这k个类别中的任意一个，因而累加所有的k个类别。
- 当二分类时，$G=2p_k(1-p_k)$

样本集合D的基尼指数：假设集合中有K个类别，每个类别的概率是$\frac{C_k}{D}$，其中$C_k$表示类别k的样本个数，D表示样本总数：
```math
Gini(D) = 1-\sum_{k=1}^k(\frac{C_k}{D})^2
```

##### 1.3 对离散特征和连续特征的处理
CART作为分类树时，特征属性可以是连续类型也可以是离散类型，但观察属性(即标签属性或者分类属性)必须是离散类型。

##### 离散特征
CART分类树算法对离散值的处理，采用的思路：不停的二分离散特征。

在ID3、C4.5，特征A被选取建立决策树节点，如果它有3个类别A1,A2,A3，我们会在决策树上建立一个三叉点，这样决策树是多叉树。

CART采用的是不停的二分。会考虑把特征A分成{A1}和{A2,A3}、{A2}和{A1,A3}、{A3}和{A1,A2}三种情况，找到基尼系数最小的组合。

比如{A2}和{A1,A3}，然后建立二叉树节点，一个节点是A2对应的样本，另一个节点是{A1,A3}对应的样本。**由于这次没有把特征A的取值完全分开，各分支下的子数据集必须依旧包含该特征，该连续特征在接下来的树分支过程中可能依旧起着决定性作用。后面还有机会对子节点继续选择特征A划分A1和A3。这和ID3、C4.5不同，在ID3或C4.5的一颗子树中，离散特征只会参与一次节点的建立。
##### 连续特征
CART分类树算法对连续值的处理，思想和C4.5相同，都是将连续的特征离散化。唯一区别在选择划分点时，C4.5是信息增益比，CART是基尼系数。

具体思路：m个样本的连续特征A有m个，从小到大排列$a_1,a_2,.....a_m$,则CART取相邻两样本值的平均数做划分点，一共取m-1个，其中第i个划分点Ti表示为：$T_i =(a_i+a_i+1)/2$。分别计算以这m-1个点作为二元分类点时的基尼系数。选择基尼系数最小的点为该连续特征的二元离散分类点。比如取到的基尼系数最小的点为，则小于的值为类别1，大于的值为类别2，这样就做到了连续特征的离散化。另外，对于连续属性先进行排序（升序），只有在决策属性（即分类发生了变化）发生改变的地方才需要切开，这可以显著减少运算量。

注意的是，与ID3、C4.5处理离散属性不同的是，如果当前节点为连续属性，则该属性在后面还可以参与子节点的产生选择过程。

##### 构建CART流程
```javascript
算法从根节点开始，用训练集递归建立CART分类树。
输入:训练集D，基尼系数的阈值，样本个数阈值。
输出:决策树T。
1）对于当前节点的数据集为D，如果样本个数小于阈值或没有特征，则返回决策子树，当前节点停止递归。
2）计算样本集D的基尼系数，如果基尼系数小于阈值，则返回决策树子树，当前节点停止递归。
3）计算当前节点现有的各个特征的各个特征值对数据集D的基尼系数。
4）在计算出来的各个特征的各个特征值对数据集D的基尼系数中，选择基尼系数最小的特征A和对应的特征值a。根据这个最优特征和最优特征值，把数据集划分成两部分D1和D2，同时建立当前节点的左右节点，做节点的数据集D为D1，右节点的数据集D为D2。
5）对左右的子节点递归的调用1-4步，生成决策树。
```
对生成的决策树做预测的时候，假如测试集里的样本A落到了某个叶子节点，而节点里有多个训练样本。则对于A的类别预测采用的是这个叶子节点里概率最大的类别。