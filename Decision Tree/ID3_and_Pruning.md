<!--
 * @Author: your name
 * @Date: 2019-12-21 09:02:00
 * @LastEditTime : 2019-12-21 11:03:56
 * @LastEditors  : Please set LastEditors
 * @Description: In User Settings Edit
 * @FilePath: /Machine_Learning/Decision Tree/Decision.md
 -->
#### 构建ID3决策树
##### 1.介绍

ID3算法是一种分类预测算法，算法以信息论中的“信息增益”为基础。核心是通过计算每个特征的信息增益，每次划分选取信息增益最高的属性为划分标准，递归地构建决策树。
ID3相当于用极大似然法进行概率模型的选择。

##### 2.具体方法

- 从根结点(root node)开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征。
- 由该特征的不同取值建立子节点，再对子结点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止；
- 最后得到一颗决策树

从ID3的构建树过程而言，它可以看成使用贪心算法得到近似最优的一颗决策树，它无法保证是最优的。
以下是代码流程:

```javascript
算法：createTree(dataSet,featList,bestFeatLists)。由给定的训练数据产生一棵判定树。
输入：
    dataSet：训练数据集
    featList：分类属性标签
    bestFeatLists：存储选择的最优特征标签
输出：
    myTree：一棵判定树。
方法：
createTree(dataSet,featList,bestFeatLists)
1）从传入的数据集dataSet中切割出分类标签，yList
2）如果yList中只有同一种标签，说明已经递归到分类边界了，则返回该标签
3）如果已经处理了dataSet中所有属性（列），但是类标签依然不是唯一的，采用多数判决的方法决定该子节点的分类
4）找出dataSet最优划分（信息增益最大）的特征所在位置bestFeatVec
5）在分类属性标签featList找出该位置所对应的特征值bestFeatLabel，并将该特征值存储到bestFeatLists中
6）将最优划分特征值作为当前（子）树的根节点，生成初始决策树myTree（用字典表示一个树结构）
7）在featList中删除当前已经使用过的特征标签（因为每次选择特征作为条件，dataSet会删掉这一列，形成新的子类，因此对应的featList中的值也要删掉）
8）确定子树分支：获取已选择的最优划分特征所对应的值分类categories（如“年龄”是最优特征，则“老”“中”“青”三个子类）
9）遍历每一个当前特征下的子类，在每个子类中，递归地调用创建决策树的方法，将递归调用的结果作为当前树节点的一个分支（构建树的方法是：特征作为字典的key，所得到的分类结果作为value；子树进行嵌套）
```

具体代码在源文件中有详细的注释。

##### 3.优缺点
1.优点
- 对于决策树，数据的准备往往是简单或者是不必要的 . 其他的技术往往要求先把数据一般化，比如去掉多余的或者空白的属性
- 能够同时处理数据型和常规型属性。其他的技术往往要求数据属性的单一。
- 是一个白盒模型如果给定一个观察的模型，那么根据所产生的决策树很容易推出相应的逻辑表达式。
- 易于通过静态测试来对模型进行评测。表示有可能测量该模型的可信度。
- 在相对短的时间内能够对大型数据源做出可行且效果良好的结果
  
2.不足
- 没有剪枝过程，为了去除过渡数据匹配的问题，可通过裁剪合并相邻的无法产生大量信息增益的叶子节点;
- 信息增益的方法偏向选择具有大量值的属性，也就是说某个属性特征索取的不同值越多，那么越有可能作为分裂属性，这样是不合理的；
- 只可以处理离散分布的数据特征。
- ID3算法只考虑了树的生成，即尽可能的是模型拟合当前训练数据集，所以该算法生成的树容易过拟合

C4.5算法是对ID3算法的改进，C4.5算法是数据挖掘十大算法之一。
改进：

- 用信息增益比来选择属性
- 在决策树的构造过程中对树进行剪枝
- 对非离散数据也能处理
- 能够对不完整数据进行处理

****

#### 决策树的剪枝
##### 1.为什么要进行剪枝？
决策树是依据训练集进行构建的，为了尽可能正确地分类训练样本，结点划分过程将不断重复，有时会造成决策树分支过多。这就可能会把训练样本学的“太好”了，以至于把训练集自身的一些特点当作所有数据都具有的一般性质而导致过拟合。因此可主动去掉一些分支来降低过拟合风险。决策树非常容易产生过拟合，实际所有非参数学习算法，都非常容易产生过拟合。

决策树的修剪，也就是剪枝操作，主要分为两种：
1. 预剪枝
2. 后剪枝

##### 1.1预剪枝
预剪枝是指在决策树生成过程中，对每个节点在划分前先进行估计，若当前节点的划分不能带来决策树泛化性能的提升，则停止划分并将当前节点标记为叶节点。
那么所谓的“决策树泛化性能”如何来判定呢？这就可以使用性能评估中的留出法，即预留一部分数据用作“验证集”以进行性能评估。
以下是预剪枝头的过程，从节点1开始，判断划分先后泛化性能。泛化指的是用验证集来测试决策树的精度。

![预剪纸](../Decision&#32;Tree/pre-Pruning.png)

##### 1.2后剪枝
后剪枝是先从训练集生成一颗完整的决策树，然后自底向上地对非叶节点进行考察，若将该节点对应的子树完全替换为叶节点能带来决策树繁花性的提升，则将该子树替换为叶节点。
下图为后剪枝过程：
![后剪纸](../Decision&#32;Tree/post.png)
后剪枝算法首先考察上图中的结点 (6)，若将以其为根节点的子树删除，即相当于把结点 (6) 替换为叶结点，替换后的叶结点包括编号为{7,15}的训练样本，因此把该叶结点标记为“好瓜”（因为这里正负样本数量相等，所以随便标记一个类别），因此此时的决策树在验证集上的精度为57.1%（未剪枝的决策树为42.9%），所以后剪枝策略决定剪枝。
![后剪纸](../Decision&#32;Tree/post1.png)
接着考察结点 5，同样的操作，把以其为根节点的子树替换为叶结点，替换后的叶结点包含编号为{6,7,15}的训练样本，根据“多数原则”把该叶结点标记为“好瓜”，测试的决策树精度认仍为57.1%，所以不进行剪枝。
考察结点 2 ，和上述操作一样，不多说了，叶结点包含编号为{1,2,3,14}的训练样本，标记为“好瓜”，此时决策树在验证集上的精度为71.4%，因此，后剪枝策略决定剪枝。
接着考察结点 3 ，同样的操作，剪枝后的决策树在验证集上的精度为71.4%，没有提升，因此不剪枝；对于结点 1 ，剪枝后的决策树的精度为42.9%，精度下降，因此也不剪枝。

##### 1.3总结
对比预剪枝和后剪枝，能够发现，后剪枝决策树通常比预剪枝决策树保留了更多的分支，一般情形下，后剪枝决策树的欠拟合风险小，泛华性能往往也要优于预剪枝决策树。但后剪枝过程是在构建完全决策树之后进行的，并且要自底向上的对树中的所有非叶结点进行逐一考察，因此其训练时间开销要比未剪枝决策树和预剪枝决策树都大得多。